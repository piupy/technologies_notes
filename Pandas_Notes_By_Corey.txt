
>>>>>>>>>>>>>>>>>>>>>> pandas notes <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

pip install pandas : to install pandas

import pandas as pd

df = pd.read_csv('data/survey_results_public.csv')

df.shape : to get the rows and columns

df.size : to get the total cell values

df.info() : to get the names and data types of all the columns

pd.set_option('display.max_columns',85) : to get all the columns printed 

pd.set_option('display.max_rows',85) : to get all the rows printed 

pd.options.display.max_columns : to get the set values

pd.reset_option('display.max_rows') / pd.reset_option('display.max_columns') : to reset the values

df.head() : top 5 rows

df.head(3) : top 3 rows

df.tail() : bottom 5 rows

df.tail(3) : bottom 3 rows


people = {
    "first": ["Corey", 'Jane', 'John'], 
    "last": ["Schafer", 'Doe', 'Doe'], 
    "email": ["CoreyMSchafer@gmail.com", 'JaneDoe@email.com', 'JohnDoe@email.com']
}

df = pd.DataFrame(people)

df['email'] : a series (column)
or
df.email : (not recommended)


df['email'][0] : 'CoreyMSchafer@gmail.com'
or
df.email[0] : (not recommended)


df[['first','last']] : to get multiple columns 

df.columns : to get the list of all the columns


>>>>>>>>>>>>> "iloc" (using integer indexes to access the columns) <<<<<<<<<<

Row index    >>> exclusive
Column index >>> exclusive

df.iloc[0] : to get the integer based row 

df.iloc[0,1] : to get the value at [0,1] place

df.iloc[[0,1]] : to get multiple rows (0th and 1st)

df.iloc[[0,2]] : to get multiple rows (0th and 2nd)

df.iloc[:3] : to get first 3 rows

df.iloc[:3,:2] : to get first 3 rows and first 2 columns


>>>>>>>>>>>>> "loc" (using column names to access the columns) <<<<<<<<<<<<<<<

Row index    >>> inclusive
Column name  >>> inclusive

df.loc[0] : to get the integer based row 

df.loc[0,'last'] : to get the value at 0th row and 'last' column

df.loc[[0,1]] : to get multiple rows (0th and 1st)

df.loc[[0,2]] : to get multiple rows (0th and 2nd)

df.loc[:3] : to get first 4 rows (in "loc", "end" is inclusive)

df.loc[:3,'first'] : (top 4 rows and "first" column)

df.loc[:3,['first','last']]  >> first 4 rows, columns with the names 'first' and 'last'


*** while accessing a column, an integer index with "loc" or using column name with "iloc" is not allowed ***


df['Hobbyist'].value_counts() : to get the value with their counts 

df.loc[:3,'first'] is recommended over df.loc[:3][:2]


>>>>>>>>>>>>>> importance of "end" parameter being inclusive in case of "loc" <<<<<<<<<<<<<<<<<<<<<<


df.loc[:2,'Hobbyist':'Employment'] : to get the columns from 'Hobbyist' to 'Employment'

df.loc[:2,'Employment':'Hobbyist':-1] : to get the columns from 'Employment' to 'Hobbyist' (in reverse 
					direction)

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

df.set_index('email') : to set the index manually temporarily

df.set_index('email',inplace=True) : to set the index manually on a permanent basis
or
df = df.set_index('email')

df.index : to see the index

Once, email becomes the key, it can't be used to get all the values of 'email' column i.e.

df['email'] will be invalid then.

df.loc['roshaa@gmail.com','fname'] : to get the first name having 'roshaa@gmail.com' as the email

df.reset_index() : to reset the index temporarily

df.reset_index(inplace=True) : to reset the index permanently
or
df = df.reset_index()


>>>>>>>> creating indexes right when reading the CSV file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df = pd.read_csv('data/survey_results_public.csv',index_col='Respondent')

df.loc[1] : first row (in this case)

however, "iloc" will still use indexes starting from 0.

df.sort_index() : to sort the index in ascending order temporarily

df.sort_index(ascending=False) : to sort the index in descending order temporarily

df.sort_index(ascending=False,inplace=True) : to sort the index in descending order permanently
or
df = df.sort_index(ascending=False)


>>>>>>>>>>>>>>>>>>>>>>>> Applying filters <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


filt = (df['last'] == 'Doe')
df[filt]

All the rows for which True was returned will be retrieved

another way is to use "loc"

df.loc[filt]
or
df.loc[filt,'email']


>>>>>>>>>>>>>>>>>>>>>>>> Logical operators (AND, OR, NOT) <<<<<<<<<<<<<<<<<<<<<<<<<

& : AND

| : OR

~ : NOT

filt = (df['last'] == 'Doe') & (df['first'] == 'John')            # AND

filt = (df['last'] == 'Doe') | (df['first'] == 'John')       	  # OR

filt = ~ ( (df['last'] == 'Doe') | (df['first'] == 'John') )      # NOT
or
df.loc[~filt,'email']            				  # NOT (~ is used for negation)



filterr = (df['ConvertedComp'] > 70000)        # having salary greater than 70000

df.loc[filterr,['ConvertedComp','LanguageWorkedWith']]


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> IN operator <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


countries = ['United Kingdom','United States','India']
filterr = df['Country'].isin(countries)			  # using "in" operator 
df.loc[filterr,'Country']		 		 


>>>>>>>>>>>>>>>>>>>>>>>>>>> LIKE operator <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

filterr = df['LanguageWorkedWith'].str.contains('Python',na=False,case=False)
df.loc[filterr,'LanguageWorkedWith']


>>>>>>>>>>>>>>>>>>>>>>>>>>> Renaming columns <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df.columns : to get the columns

df.columns = ['abc','pqr','xyz']  :  in case all the names need to be renamed

df.columns = [x.upper() for x in df.columns] : to change the column names into uppercase

df.columns = df.columns.str.replace(' ','_') : to replace spaces with underscore

df.rename(columns={'first' : 'first_name', 'last' : 'last_name'},inplace=True) : to rename specific 
										 columns


>>>>>>>>>>>>>>>>>>>>>> Updating the values <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


df.loc[2] = ['abc','mno','pqr','xyz']  : if the whole row needs to be updated

df.loc[2,'first'] = 'abc'   :   if just the single value needs to be updated

df.loc[2,['first','last']] = ['abc','xyz']   :   if multiple values need to be updated

df.at[2,'first'] = 'abc'   :   if just the single value needs to be updated (faster than "loc")

"iat" is also available

df.iat[2,3] = 'abc'   :   if just the single value needs to be updated (faster than "iloc")


>>>>>>>>>>>>>>>>>>>>>>>>>> "apply", "applymap", "map", "replace" functions <<<<<<<<<<<<<<<<<<<<<<<<<

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> apply <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

To update a column's value with a certain value. A function name needs to be passed which will be 
called for each value of that column


def email_up(email):
    return email.upper() 


df['email'] = df['email'].apply(email_up)


Instead, a lambda function can also be used

df['email'] = df['email'].apply(lambda x : x.lower())


df.apply(pd.Series.min) :  to get the minimum values of all the columns 
or
df.apply(lambda x : min(x))
or
df.apply(lambda x : x.min())


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> applymap <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

To update the values of all the rows and columns 

df.applymap(len) : gives the length of all the values of the DataFrame

df.applymap(str.lower) : gives all the values of the DataFrame in lowercase
or
df.applymap(lambda x : x.lower()) :  using a lambda function


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> map <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['first'].map({'corey' : 'chris', 'jane' : 'mary'}) : to replace the values. If a value is not 
changed, it will be converted to np.NaN.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> replace <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['first'].replace({'corey' : 'chris', 'jane' : 'mary'},inplace=True) : to replace the values. If a
value is not changed, it will remain same.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> adding a column <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['full_name'] = df['first'] + ' ' + df['last']


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> dropping a column <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df.drop(columns=['marks'],inplace=True)


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> adding new columns from existing columns <<<<<<<<<<<<<<<<<<<<<<<<<

df[['first','last']] = df['full_name'].str.split(expand=True)


>>>>>>>>>>>>>>>>>>>>>>>> getting the cell values length <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['first'] + '_' + df['last'] + '_' + (df['first'].apply(len) + df['last'].apply(len)).apply(str)

df['last'].apply(len).apply(str) :  to convert int values into str

df['first'].to_frame() : to convert a series into a DataFrame


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> adding a row <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df = df.append({ 'first' : 'Tony', 'last' : 'Stark' },ignore_index=True)


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> deleting a row <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df.drop(3,inplace=True) :  "3" is the index
or
df.drop([1,2],inplace=True)


deleting a row based on a condition :

filterr = (df['last'] == 'Doe')
df.drop(df[filterr].index)


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> sorting <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df.sort_values(by='last') :  to sort in ascending order

df.sort_values(by='last',ascending=False) :  to sort in descending order

df.sort_values(by=['last','first'],ascending=False) :  to sort in descending order first by last_name
						       and then by first_name

df.sort_values(by=['last','first'],ascending=[False,True]) :  to sort in descending order by last_name
						              but in ascending order by first_name

df.sort_values(by=['last','first'],ascending=[False,True],inplace=True) : to make changes permanent


df.sort_index() : to sort based on the indexes in ascending order

df.sort_index(ascending=False) : to sort based on the indexes in descending order

df['last'].sort_values() : to sort the Series values in ascending order

df['last'].sort_values(ascending=False) : to sort the Series values in descending order

If sorting is applied on a Series, it will not be shown while displaying the DataFrame. It will only 
be visible when the Series is displayed


df['salary'].nlargest(10) : 10 highest salaries

df['salary'].nsmallest(10) : 10 smallest salaries

df.nlargest(10,'salary') : all the column values having 10 highest salaries

df.nsmallest(10,'salary') : all the column values having 10 smallest salaries


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> grouping <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['ConvertedComp'].median() : to get the median 

df.median() :  to get the median of the numerical columns

df.describe() : to get the mean,count,median and other things in a single shot of all the rows

df['ConvertedComp'].describe() : to get the mean,count,median and other things in a single shot of a 
				 particular Series

df['ConvertedComp'].count() : gives the count of non-missing values

df['Hobbyist'].value_counts() : gives the count of the values 

df['Hobbyist'].value_counts(normalize=True) * 100 : gives the count of the values in %


grp_US = df.groupby(['Country']).get_group('United States')
grp_US['SocialMedia'].value_counts()

alternatively, by using filters

filterr = (df['Country'] == 'United States')
df[filterr]['SocialMedia'].value_counts()       


getting the values of all the groups :

grp_country = df.groupby(['Country'])
grp_country['SocialMedia'].value_counts()


getting the values of all the groups of a particular country:

grp_country = df.groupby(['Country'])
grp_country['SocialMedia'].value_counts().loc['India']


in % : 

grp_country = df.groupby(['Country'])
grp_country['SocialMedia'].value_counts(normalize=True).loc['India'] * 100


Mean, median, sum, count, min, max of the groups :

grp_country = df.groupby(['Country'])
grp_country['ConvertedComp'].mean()
grp_country['ConvertedComp'].median()
grp_country['ConvertedComp'].sum()
grp_country['ConvertedComp'].count()
grp_country['ConvertedComp'].min()
grp_country['ConvertedComp'].max()

of a particular country : 

grp_country['ConvertedComp'].max().loc['India']

to get the aggregations together :

grp_country['ConvertedComp'].agg(['median','mean','sum','count','min','max']).loc['India']


>>>>> count of rows having "Python" in the "LanguageWorkedWith" Column (similar to "LIKE" operator) :

by creating a filter : (it will give just for a single country)

filterr = (df['Country'] == 'India')
df[filterr]['LanguageWorkedWith'].str.contains('Python').sum()


by creating a group, "apply" function needs to be used : (gives for each group)

grp_country = df.groupby(['Country'])
grp_country['LanguageWorkedWith'].apply(lambda x : x.str.contains('Python').sum())


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> % of the values <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

To concatenate two Series :

percent_df = pd.concat([percent,total,percentage],axis='columns')


grp_country = df.groupby(['Country'])
percent = grp_country['LanguageWorkedWith'].apply(lambda x : x.str.contains('Python').sum())
total = df['Country'].value_counts()
percentage = (percent/total) * 100
percent_df = pd.concat([percent,total,percentage],axis='columns')    # axis=1 can also be used
percent_df


axis ->  1 : columns
	 0 : row (index)

>>>>>>>>>>>>>>>>>>>>>>>>>>>>> handling missing values <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

import numpy as np

people = {
    'first': ['Corey', 'Jane', 'John', 'Chris', np.nan, None, 'NA'], 
    'last': ['Schafer', 'Doe', 'Doe', 'Schafer', np.nan, np.nan, 'Missing'], 
    'email': ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', None, np.nan, 'Anonymous@email.com', 'NA'],
    'age': ['33', '55', '63', '36', None, None, 'Missing']
}

df = pd.DataFrame(people)


df.dropna(axis='index',how='any') :  default parameters. 

axis : to search across either index(rows) or columns.

	default value of "axis" is "index" or "0"

how : how we want to search for None values. 
	
	"any" means if any value across axis is found None, drop that axis.
	"all" means if all the values across axis are found None, then only drop that axis.
	
	default value of "how" is "any"


To check for some particular axes only, use "subset" attribute, i.e. if that particular axis is
missing, drop that axis :

df.dropna(axis='index',how='any',subset=['email'])


>>> When 1 axis is passed :

If "subset" is used, "how" is of no importance if there is only 1 axis in the subset list. It may be 
set to "any" or "all", it will not decide whether to drop that axis or not. It is just like 
prioritising a particular axis.

>>> When more than 1 axes are passed : 

df.dropna(axis='index',how='any',subset=['last','email'])

"how" in case of "any" works like "OR" operator i.e. if any of the axes present in the list are 
missing, drop that axis


df.dropna(axis='index',how='all',subset=['last','email'])

"how" in case of "all" works like "AND" operator i.e. if all the axes present in the list are missing,
drop that axis


df.dropna(axis='index',how='all',subset=['last','email'],inplace=True) : to make changes permanent


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Replacing the missing values <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df.replace('NA',np.nan,inplace=True)
df.replace('Missing',np.nan,inplace=True)

df.isna() : to check for nan values 

df.fillna('MISSING') : to fill the nan values
or
df.fillna(0)

df['age'].fillna(0,inplace=True) : to make changes permanent


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> typecasting a Series <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['age'].dtypes : to get the datatype of a Series

df['age'] = df['age'].astype(float)   :  to typecast into a particular type
or
df['age'] = df['age'].apply(float)


df.astype(float)  : to typecast the whole DataFrame values into a particular type 


converting the values to a nan as soon as the file is read : 

na_vals = ['NA','MISSING']
df = pd.read_csv('data/survey_results_public.csv',na_values=na_vals)


It will replace the na values given in the list and convert them to nan automatically while reading 
the file.

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> unique values method <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['YearsCode'].unique()

Series of type Object needs to be converted into float to get the average

df['YearsCode'] = df['YearsCode'].astype(float)

df['YearsCode'].mean()


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> handling datetime values <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

df['Date'] = pd.to_datetime(df['Date'],format='%Y-%m-%d %I-%p')

df.loc[0,'Date'].day_name() : to get the day name

supplying the details of the date type columns as soon as the file is read 


from datetime import datetime
d_parser = lambda x : datetime.strptime(x,'%Y-%m-%d %I-%p')
df = pd.read_csv('data/ETH_1h.csv',parse_dates=['Date'],date_parser=d_parser)


df['Date'].dt.day_name() : to get the day names of all the values 

df['Date'].min() : to get the minimum date (most historical date)

df['Date'].max() : to get the maximum date (latest date)

df['Date'].max() - df['Date'].min() : to get the day difference in the form of a Timedelta


>>> filtering the dates 

filterr = (df['Date'] >= '2020')
df.loc[filterr]


>>> filtering the dates of a certain year

filterr = (df['Date'] >= '2019') & (df['Date'] < '2020')
df.loc[filterr]

or 

filterr = (df['Date'] >= pd.to_datetime('2019-01-01')) & (df['Date'] < pd.to_datetime('2020-01-01'))
df.loc[filterr]


df.set_index('Date',inplace=True) : to set the date as the index

df.loc['2019'] : to get the data of 2019

df.loc['2020-01':'2020-02'] : to get the data from Jan 2020 to Feb 2020


>>> resampling (keeping track of all the highest values on a daily basis)

highs = df['High'].resample('D').max()
highs
highs['2020-01-01']


pip install matplotlib

%matplotlib inline

highs.plot() : to plot the values


resampling can be done on weekly basis as well.

df.resample('W').mean()


to run different aggreagation functions on different columns :

df.resample('W').agg({ 'Close' : 'mean', 'High' : 'max', 'Low' : 'min', 'Volume' : 'sum' })


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> writing into .csv file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


filterr = (df['Country'] == 'India')
india_df = df.loc[filterr].head()
india_df.to_csv('data/modified.csv')


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> reading a .csv file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

ind_df = pd.read_csv('data/modified.csv')


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> writing into .tsv file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


filterr = (df['Country'] == 'India')
india_df = df.loc[filterr].head()
india_df.to_csv('data/modified.tsv',sep='\t')


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> reading a .tsv file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

ind_df = pd.read_csv('data/modified.tsv',sep='\t')


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> writing an .xls file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

pip install xlwt xlrd openpyxl

filterr = (df['Country'] == 'India')
india_df = df.loc[filterr]
india_df.to_excel('data/modified.xlsx')
or
india_df.to_excel('data/modified.xlsx',index=False) : in case index need not to be written


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> reading an .xls file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

ind_df = pd.read_excel('data/modified.xlsx',index_col='Respondent')



>>>>>>>>>>>>>>>>>>>>>>>>>>>>> writing a .json file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

filterr = (df['Country'] == 'India')
india_df = df.loc[filterr]
india_df.to_json('data/modified.json')
or
india_df.to_json('data/modified.json',orient='records') : in case index need not to be written


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> reading a .json file <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

ind_df = pd.read_json('data/modified.json')


>>>>>>>>>>>>>>>>>>>>>>>>>>>>> writing an .sql table <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

pip install SQLAlchemy

pip install psycopg2-binary  # if working with postgres

from sqlalchemy import create_engine
import psycopg2
engine = create_engine('postgresql://dbuser:dbpass@localhost:5432/sample_db')
india_df.to_sql('sample_table',engine,if_exists='replace')

>>>>>>>>>>>>>>>>>>>>>>>>>>>>> reading an .sql table <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

sql_df = pd.read_sql('sample_table',engine,index_col='Respondent')


>>>>>>>>>>>>>>>>>>>>>> reading an .sql table from a specific query <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

sql_df = pd.read_sql_query('select * from sample_table where respondent <= 10',engine,index_col='Respondent')


>>>>>>>>>>>>>>>>>>> retrieving username and password from the environment variables <<<<<<<<<<<<<<<<

import os 
username,password = os.environ.get('db_user'),os.environ.get('db_pass')
print(f'username : {username} , password : {password}')


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> merge (joins) <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

merge can be used to join two dataframes

data1 = [
    
    { "id" : 1,
        "name" : "Rohit",
        "salary" : 50000,
     "address" : "Delhi"
    },
    { "id" : 2,
        "name" : "Amit",
        "salary" : 60000,
     "address" : "Delhi"
    },
    { "id" : 3,
        "name" : "Sumit",
        "salary" : 80000,
     "address" : "Delhi"
    }
    
]

data2 = [
    
    { "id" : 1,
        "name" : "X",
        "salary" : 50000,
     "address" : "Goa"
    },
    { "id" : 2,
        "name" : "Y",
        "salary" : 60000,
     "address" : "Goa"
    },
    { "id" : 4,
        "name" : "Z",
        "salary" : 80000,
     "address" : "Agra"
    }
    
]

df1 = pd.DataFrame(data1)
df2 = pd.DataFrame(data2)
df3 = pd.merge(df1,df2,how="inner", left_on="id", right_on="id")

"how" can be "inner", "left", "right", "outer"


However, "join" function also exists but "merge" function is more flexible.


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> concat <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

https://github.com/codebasics/py/blob/master/pandas/8_concat/pandas_concat.ipynb


india_weather = pd.DataFrame({
    "city": ["mumbai","delhi","banglore"],
    "temperature": [32,45,30],
    "humidity": [80, 60, 78]
})


us_weather = pd.DataFrame({
    "city": ["new york","chicago","orlando"],
    "temperature": [21,14,35],
    "humidity": [68, 65, 75]
})

df = pd.concat([india_weather, us_weather], ignore_index=True) 	    # added as row by default

df = pd.concat([india_weather, us_weather], ignore_index=True, axis=1) 	    # added as column

df = pd.concat([temperature_df,pd.Series(["Humid","Dry","Rain"], name="event")],axis=1)  # to add a series as a column


>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> End of Pandas <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<


